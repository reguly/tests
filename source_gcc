#!/bin/bash
#OneAPI Install paths
module load nvhpc/23.7
#source /bask/projects/j/jarvissz-cfd-scaling/oneapi2/setvars.sh --force
#export HDF5_INSTALL_PATH=/bask/apps/test/EL8-ice/software/HDF5/1.10.7-iimpi-2021a
export MPI_INSTALL_PATH=$(dirname $(dirname `which mpicxx`))
export CUDA_INSTALL_PATH=/opt-local/nvidia/hpc_sdk/Linux_x86_64/23.7/cuda/12.2/
export CUDA_PATH=/opt-local/nvidia/hpc_sdk/Linux_x86_64/23.7/cuda/12.2/
#test suite
BASE_DIR=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
export OPS_COMPILER=gnu
export OP2_COMPILER=gnu
export OPS_INSTALL_PATH=$BASE_DIR/OPS/ops
export OP2_INSTALL_PATH=$BASE_DIR/OP2-Common/op2
export HDF5_INSTALL_PATH=$BASE_DIR/hdf5-seq
export HDF5_SEQ_INSTALL_PATH=$BASE_DIR/hdf5-seq
export HDF5_PAR_INSTALL_PATH=$BASE_DIR/hdf5
export PTSCOTCH_INSTALL_PATH=$BASE_DIR/ptscotch
export PARMETIS_INSTALL_PATH=$BASE_DIR/parmetis
export I_MPI_CC=gcc
export I_MPI_CXX=g++
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HDF5_INSTALL_PATH/lib
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HDF5_PAR_INSTALL_PATH/lib
export TMPDIR=`pwd`
export OMPI_CC=$I_MPI_CC
export OMPI_CXX=$I_MPI_CXX
export NV_ARCH=Hopper

export bind_numa="-bind-to numa"
#this is just for the OpenMPI setup on PPCU local cluster
# export mpirunflags="-mca io ^ompio"
# export bind_numa="~/numawrap_omp2"


