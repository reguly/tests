#!/bin/bash
#OneAPI Install paths
#source /bask/projects/j/jarvissz-cfd-scaling/oneapi2/setvars.sh --force
#export HDF5_INSTALL_PATH=/bask/apps/test/EL8-ice/software/HDF5/1.10.7-iimpi-2021a
module load CrayEnv
module load PrgEnv-cray
module load craype-accel-amd-gfx90a
module load rocm
export HIP_INSTALL_PATH=/opt/rocm-5.2.3
export ROCM_PATH=$HHIP_INSTALL_PATH
export MPI_INSTALL_PATH=$(dirname $(dirname `which cc`))

#test suite
BASE_DIR=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
export OPS_COMPILER=cray
export OP2_COMPILER=cray
export OPS_INSTALL_PATH=$BASE_DIR/OPS/ops
export OP2_INSTALL_PATH=$BASE_DIR/OP2-Common/op2
export HDF5_INSTALL_PATH=$BASE_DIR/hdf5
export HDF5_SEQ_INSTALL_PATH=$BASE_DIR/hdf5-seq
export PTSCOTCH_INSTALL_PATH=$BASE_DIR/ptscotch
export PARMETIS_INSTALL_PATH=$BASE_DIR/parmetis
export I_MPI_CC=clang
export I_MPI_CXX=clang++
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HDF5_INSTALL_PATH/lib:$HDF5_SEQ_INSTALL_PATH/lib
#export TMPDIR=`pwd`
export OMPI_CC=$I_MPI_CC
export OMPI_CXX=$I_MPI_CXX
export HIP_ARCH=gfx90a

export bind_numa="-bind-to numa"
#this is just for the OpenMPI setup on PPCU local cluster
# export mpirunflags="-mca io ^ompio"
# export bind_numa="~/numawrap_omp2"


